version: "3"

services:
  llama:
    image: ghcr.io/ggerganov/llama.cpp:full
    pull_policy: always
    command: --server --host 0.0.0.0 --port 8080 --path /public --model /models/llama-2-7b-chat.Q4_K_M.gguf --embedding --alias default
    volumes:
      - ./models:/models:cached
  
  sentence-transformers:
    image: semitechnologies/transformers-inference:sentence-transformers-multi-qa-MiniLM-L6-cos-v1
    pull_policy: always
    environment:
      ENABLE_CUDA: 0

  gateway:
    image: adrianliechti/llama-openai:2
    build:
      context: .
      dockerfile: Dockerfile
    pull_policy: always
    ports:
      - 8080:8080
    configs:
      - source: config
        target: /config.yaml
  
  web:
    image: ghcr.io/mckaywrigley/chatbot-ui:main
    pull_policy: always
    ports:
      - 3000:3000
    environment:
      - OPENAI_API_KEY=changeme
      - OPENAI_API_HOST=http://gateway:8080/oai
      - DEFAULT_MODEL=gpt-3.5-turbo
  
configs:
  config:
    content: |
      providers:
        - type: llama
          url: http://llama:8080
          models:
            gpt-3.5-turbo:
              id: default
              template: llama
        
        - type: sentence-transformers
          url: http://sentence-transformers:8080
          models:
            text-embedding-ada-002:
              id: all-minilm-l6-v2

      authorizers:
        - type: static
          token: changeme