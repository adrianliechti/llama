# https://taskfile.dev

version: "3"

tasks:
  server:
    deps: [ download-model ]
    cmds:
      - llama-server 
        --port 9081
        --log-disable
        --ctx-size 32768
        --flash-attn
        --model ./models/llama-3.2-3b-instruct.gguf
  
  download-model:
    cmds:
      - mkdir -p models
      - curl -s -L -o models/llama-3.2-3b-instruct.gguf https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_L.gguf?download=true

    status:
      - test -f models/llama-3.2-3b-instruct.gguf
  
  test:
    cmds:
      - |
        curl http://localhost:9081/v1/chat/completions \
          -H "Content-Type: application/json" \
          -d '{
            "model": "llama",
            "messages": [
              {
                "role": "user",
                 "content": "Hello!"
              }
            ]
          }'