# https://taskfile.dev

version: "3"

vars:
  REPOSITORY: adrianliechti/llama-platform

tasks:
  publish:
    cmds:
      - docker buildx build . --push --platform linux/amd64,linux/arm64 --tag {{.REPOSITORY}}

  server:
    cmds:
      - go run cmd/server/main.go

  client:
    cmds:
      - go run cmd/client/main.go

  webui:
    cmds:
      - docker run -it --rm -p 3000:3000 -e OPENAI_BASE_URL=http://host.docker.internal:8080/oai/v1 -e OPENAI_MODEL=mistral adrianliechti/llama-ui

  llama-server:
    deps: [download-llama-server, download-llama-model]
    cmds:
      - bin/llama-server --port 9081 --log-disable --ctx-size 8192 --embedding --model ./models/mistral-7b-instruct-v0.2.Q4_K_M.gguf

  download-llama-model:
    cmds:
      - mkdir -p models
      - curl -s -L -o models/mistral-7b-instruct-v0.2.Q4_K_M.gguf https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf

    status:
      - test -f models/mistral-7b-instruct-v0.2.Q4_K_M.gguf

  download-llama-server:
    cmds:
      - mkdir -p bin
      - rm -rf tmp/llama.cpp
      - git clone https://github.com/ggerganov/llama.cpp tmp/llama.cpp
      - make -C tmp/llama.cpp/ server
      - cp tmp/llama.cpp/server bin/llama-server
      - cp tmp/llama.cpp/ggml-metal.metal bin/ggml-metal.metal
      - rm -rf tmp/llama.cpp
      - rm -rf tmp

    status:
      - test -f bin/llama-server

  whisper-server:
    deps: [download-whisper-server, download-whisper-model]
    cmds:
      - bin/whisper-server --port 9085 --convert --model ./models/whisper-ggml-medium.bin

  download-whisper-model:
    cmds:
      - mkdir -p models
      - curl -s -L -o models/whisper-ggml-medium.bin https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-medium.bin

    status:
      - test -f models/whisper-ggml-medium.bin

  download-whisper-server:
    cmds:
      - mkdir -p bin
      - rm -rf tmp/whisper.cpp
      - git clone https://github.com/ggerganov/whisper.cpp tmp/whisper.cpp
      - make -C tmp/whisper.cpp/ server
      - cp tmp/whisper.cpp/server bin/whisper-server
      - rm -rf tmp/whisper.cpp
      - rm -rf tmp

    status:
      - test -f bin/whisper-server

  sbert-server:
    cmds:
      - docker run -it --rm -p 9082:8080 semitechnologies/transformers-inference:sentence-transformers-all-mpnet-base-v2

  chroma-server:
    cmds:
      - docker run -it --rm -p 9083:8000 -v chroma-data:/chroma/chroma ghcr.io/chroma-core/chroma

  weaviate-server:
    cmds:
      - docker run -it --rm -p 9084:8080 -v weaviate-data:/var/lib/weaviate -e CLUSTER_HOSTNAME=node1 -e DEFAULT_VECTORIZER_MODULE=none -e AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=true -e PERSISTENCE_DATA_PATH=/var/lib/weaviate semitechnologies/weaviate

  unstructured-server:
    cmds:
      - docker run -it --rm -p 9085:8000 downloads.unstructured.io/unstructured-io/unstructured-api --port 8000 --host 0.0.0.0

  opensearch-server:
    cmds:
      - docker run -it --rm -p 9200:9200 -v opensearch-data:/usr/share/opensearch/data -e "discovery.type=single-node" -e DISABLE_SECURITY_PLUGIN=true opensearchproject/opensearch:latest
