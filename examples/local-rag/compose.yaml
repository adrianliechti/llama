services:
  platform:
    image: ghcr.io/adrianliechti/llama-platform
    pull_policy: always
    build:
      context: ../../
      dockerfile: Dockerfile
    ports:
      - 8080:8080
    configs:
      - source: platform
        target: /config.yaml
      - source: prompt_docs
        target: /prompt_docs.tmpl
    depends_on:
      - qdrant
      - unstructured

  qdrant:
    image: qdrant/qdrant:v1.10.1
    pull_policy: always
    volumes:
      - qdrant-data:/qdrant/storage

  unstructured:
    image: quay.io/unstructured-io/unstructured-api:0.0.75
    pull_policy: always
    command: --port 8000 --host 0.0.0.0

  web:
    image: ghcr.io/adrianliechti/llama-streamlit
    pull_policy: always
    ports:
      - 8501:8501
    environment:
      - OPENAI_BASE_URL=http://platform:8080/v1
    depends_on:
      - platform

configs:
  platform:
    content: |
      providers:
        - type: ollama
          url: http://host.docker.internal:11434
          models:
            llama:
              id: llama3.1:latest
            
            nomic-embed-text-v1:
              id: nomic-embed-text:latest

      indexes:
        docs:
          type: qdrant
          url: http://qdrant:6333
          namespace: docs
          embedding: nomic-embed-text-v1

      converters:          
        unstructured:
          type: unstructured
          url: http://unstructured:8000
          chunkSize: 4000
          chunkOverlap: 200

      chains:
        docs:
          type: rag
          index: docs
          model: llama
          template: /prompt_docs.tmpl
          limit: 5
          temperature: 0.5
  
  prompt_docs:
    file: ./prompt_docs.tmpl

volumes:
  qdrant-data:
