services:
  platform:
    image: ghcr.io/adrianliechti/llama-platform
    pull_policy: always
    build:
      context: ../../
      dockerfile: Dockerfile
    ports:
      - 8080:8080
    configs:
      - source: platform
        target: /config.yaml
    depends_on:
      - ollama
      - ollama-companion
      - qdrant
  
  ollama:
    image: ollama/ollama:0.3.12
    pull_policy: always
    volumes:
      - ollama-data:/root/.ollama
  
  ollama-companion:
    image: ghcr.io/adrianliechti/ollama-companion
    pull_policy: always
    restart: on-failure
    environment:
      - OLLAMA_HOST=ollama:11434
      - OLLAMA_MODELS=llama3.2:1b,nomic-embed-text:v1.5
  
  qdrant:
    image: qdrant/qdrant:v1.11.4
    pull_policy: always
    ports:
      - 6333:6333
    volumes:
      - qdrant-data:/qdrant/storage
  
  unstructured:
    image: quay.io/unstructured-io/unstructured-api:0.0.80
    pull_policy: always

  web:
    image: ghcr.io/adrianliechti/llama-streamlit
    pull_policy: always
    ports:
      - 8501:8501
    environment:
      - OPENAI_BASE_URL=http://platform:8080/v1
    depends_on:
      - platform

configs:
  platform:
    content: |
      providers:
        - type: ollama
          url: http://ollama:11434

          # https://ollama.com/library
          models:
            llama:
              id: llama3.2:1b
            
            nomic:
              id: nomic-embed-text:v1.5
      
      extractors:
        unstructured:
          type: unstructured
          url: http://unstructured:8000

        text:
          type: text
      
      indexes:
        docs:
          type: qdrant
          url: http://qdrant:6333
          namespace: docs
          embedder: nomic
      
      chains:
        docs:
          type: rag
          model: llama
          index: docs

volumes:
  ollama-data:
  qdrant-data: