services:
  platform:
    image: ghcr.io/adrianliechti/llama-platform
    pull_policy: always
    build:
      context: ../../
      dockerfile: Dockerfile
    ports:
      - 8080:8080
    configs:
      - source: platform
        target: /config.yaml
  
  mistral-7b-instruct:
    image: ghcr.io/huggingface/text-generation-inference:1.4
    pull_policy: always
    command: --hostname 0.0.0.0 --model-id mistralai/Mistral-7B-Instruct-v0.1
    shm_size: 1g
    volumes:
      - models-data:/data
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
              
  nomic-embed-text:
    image: ghcr.io/huggingface/text-embeddings-inference:1.2
    pull_policy: always
    command: --hostname 0.0.0.0 --model-id nomic-ai/nomic-embed-text-v1.5
    shm_size: 1g
    volumes:
      - models-data:/data
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
  
  web:
    image: ghcr.io/adrianliechti/llama-streamlit
    pull_policy: always
    ports:
      - 8501:8501
    environment:
      - OPENAI_BASE_URL=http://platform:8080/v1
    depends_on:
      - platform
  
configs:
  platform:
    content: |
      providers:
        - type: huggingface
          url: http://mistral-7b-instruct
          models:
            - mistral-7b-instruct
        
        - type: huggingface
          url: http://nomic-embed-text
          models:
            - nomic-embed-text

volumes:
  models-data: