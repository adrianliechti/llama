services:
  platform:
    image: ghcr.io/adrianliechti/llama-platform
    pull_policy: always
    build:
      context: ../../
      dockerfile: Dockerfile
    ports:
      - 8080:8080
    configs:
      - source: platform
        target: /config.yaml
  
  web:
    image: ghcr.io/adrianliechti/llama-streamlit
    pull_policy: always
    ports:
      - 8501:8501
    environment:
      - OPENAI_BASE_URL=http://platform:8080/v1
    depends_on:
      - platform
  
configs:
  platform:
    content: |
      providers:
        - type: ollama
          url: http://host.docker.internal:11434
          models:
            - gemma
            - mistral
            - llava
            
      chains:
        terminal:
          type: assistant
          model: mistral
          messages:
          - role: system
            content: You are a simulated Linux terminal. Respond to user input as if they are entering commands in a Linux terminal. Execute commands, display the expected output, and handle errors as a real Linux terminal would. Keep your responses concise and accurate, resembling the actual terminal experience. You MUST answer in Markdown using ``` blocks.
          - role: user
            content: pwd
          - role: assistant
            content: /home/user

        